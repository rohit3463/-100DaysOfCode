# -*- coding: utf-8 -*-
"""Tackle-Toxic-embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ShEd5TEXJmIdc1bthAR6UGf5n_FunK1Z
"""
#this is a recreation of a kaggle notebook originally at https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge/notebook

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':''})
downloaded.GetContentFile('train.csv')
downloaded = drive.CreateFile({'id':''})
downloaded.GetContentFile('GoogleNews-vectors-negative300.bin')

# importing the necessary libraries
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
import gensim.models.keyedvectors as word2vec
import gc

#reading the data
train = pd.read_csv('train.csv')

#division into training input and target variable

list_classes = train.columns[2:]
y = train[list_classes].values
list_train_sentences = train['comment_text']

# Tokenize the input sentences
max_features = 20000
tokenizer = Tokenizer(num_words = max_features)
tokenizer.fit_on_texts(list(list_train_sentences))
list_tokenized_train = tokenizer.texts_to_sequences(list_train_sentences)

# padding input sentences into 200 size i.e. the maximum size any sentence can have
maxlen = 200
X_t = pad_sequences(list_tokenized_train, maxlen = maxlen)


# defining a function to convert our words into vectors through pre-trained embeddings
def loadEmbedding():
  #loading the word2vec dictionary with gensim 
  word2vecDict = word2vec.KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin", binary=True)
  embed_size = 300
  
  #putting word vectors into a dictionary
  embeddings_index = dict()
  for word in word2vecDict.wv.vocab:
    embeddings_index[word] = word2vecDict.word_vec(word)
  
  gc.collect()
  print(len(embeddings_index))
  
  #finding mean and standard deviation to create embedding matrix initiating with normal distribution
  all_embs = np.stack(list(embeddings_index.values()))
  embs_mean,embs_std = all_embs.mean(),all_embs.std()
  
  nb_words = list(tokenizer.word_index)
  
  embedding_matrix = np.random.normal(embs_mean, embs_std, (nb_words, embed_size))
  gc.collect()
  
  embedded_count = 0
  
  #converting the given tokenized words into vector and storing into embedding matrix
  for word, i in tokenizer.words_index.items():
    i -= 1
    
    embedding_vector = embeddings_index[word]
    
    if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector
      embedded_count += 1
      
  del(embeddings_index)
  gc.collect()
      
  return embedding_matrix

embedding_matrix = loadEmbedding()

nb_words = len(tokenizer.word_index)

#model
inp = Input(shape=(maxlen,))
x = Embedding(nb_words, embedding_matrix.shape[1], weights = [embedding_matrix], trainable = False)(inp)
x = Bidirectional(LSTM(60, return_sequences = True, name = "lstm_layer", dropuout = 0.1, recurrent_dropout = 0.1))(x)
x = GlobalMax1D()(x)
x = Dropout(0.1)(x)
x = Dense(50, activation = "relu")(x)
x = Dropout(0.1)(x)
x = Dense(6, activation = "sigmoid")(x)

model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

#model fitting
batch_size = 32
epochs = 4
hist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)

#plotting model training accuracy with validation accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#plotting model training loss with validation loss after every epoch
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()