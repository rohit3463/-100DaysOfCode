# -*- coding: utf-8 -*-
"""Tackle-Toxic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kWSuX5IrvqB1lHI3YjG2_-UiyYzbT9im
"""
#this is a recreation of a kaggle notebook originally at https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras/data
# Google Drive Authentication
!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#import files into google colab
downloaded = drive.CreateFile({'id':''})
downloaded.GetContentFile('train.csv')
downloaded = drive.CreateFile({'id':''})
downloaded.GetContentFile('test.csv')

#import necessary libraries
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers


#putting into dataframes
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

print(train.head())

print(train.isnull().any())
print(test.isnull().any())

#creating target and train variables
list_classes = list(train.columns)[2:]
y = train[list_classes].values

list_train_sentences = train["comment_text"]
list_test_sentences = test["comment_text"]

#tokenizing the input array and converting into dictionary indices
max_features = 20000
tokenizer = Tokenizer(num_words = max_features)
tokenizer.fit_on_texts(list(list_train_sentences))
list_tokenized_train = tokenizer.texts_to_sequences(list_train_sentences)
list_tokenized_test = tokenizer.texts_to_sequences(list_test_sentences)

#padding input sentences into 200 size i.e. the maximum size any sentence can have
maxlen = 200
X_t = pad_sequences(list_tokenized_train, maxlen = maxlen)
X_te = pad_sequences(list_tokenized_test, maxlen = maxlen)

#checking for an appropriate 'maxlen'
lengths = [len(sentence) for sentence in list_tokenized_train]

plt.hist(lengths, bins = np.arange(0, 410, 10))

plt.show()

#creating the model
embed_size = 128

inp = Input(shape = (maxlen, ))
x = Embedding(max_features, embed_size)(inp)
x = LSTM(60, return_sequences = True, name = "lstm_layer")(x)
x = GlobalMaxPool1D()(x)
x = Dropout(0.1)(x)
x = Dense(50, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(6, activation="sigmoid")(x)

model = Model(inputs = inp, outputs = x)
model.compile(loss = "binary_crossentropy", optimizer = "adam", metrics = ['accuracy'])

#model fitting acquired an accuracy on validation set of split 10% = 98.32%
batch_size = 32
epochs = 2
model.fit(X_t, y, batch_size = batch_size, epochs = epochs, validation_split = 0.1)