# -*- coding: utf-8 -*-
"""Keyword-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EwEoVhHkR1KGCm4OKh5uQ5yAysn4ztMc
"""

# !pip install -U -q PyDrive

# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

# # 1. Authenticate and create the PyDrive client.
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)

# downloaded = drive.CreateFile({'id':''})
# downloaded.GetContentFile('papers_filtered.csv')

import pandas as pd
import numpy as np
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer

nltk.download('wordnet') 
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from scipy.sparse import coo_matrix

#function to extract top k words from the sorted list of tuples with (feature_index, feature tf-idf score)
def extract_topn_words(feature_names, sorted_items, topn = 5):
  sorted_items = sorted_items[:topn]
  
  score_vals = []
  feature_vals = []
  
  for idx, score in sorted_items:
    score_vals.append(round(score, 3))
    feature_vals.append(feature_names[idx])
  
  results = {}
  for idx in range(len(feature_vals)):
    results[feature_vals[idx]] = score_vals[idx]
    
  return results

#function to sort tfidf vector and making list of tuples with (feature_index, feature tf-idf score)
def sorted_coo(coo_matrix):
  tuples = zip(coo_matrix.col, coo_matrix.data)
  
  return sorted(tuples, key = lambda x: (x[1], x[0]), reverse = True)



#function to get top_n_words for ngram = 3
def get_top_n3_words(corpus, n = None):
  vec = CountVectorizer(ngram_range = (3,3)).fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis = 0)
  
  word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
  
  word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)
  
  return word_freq[:n]




#function to get top_n_words for ngram = 2
def get_top_n2_words(corpus, n = None):
  vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis = 0)
  
  word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
  
  word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)
  
  return word_freq[:n]


  

#function to get top_n_words for ngram = 1
def get_top_n_words(corpus, n = None):
  vec = CountVectorizer().fit(corpus)
  bag_of_words = vec.transform(corpus)
  sum_words = bag_of_words.sum(axis = 0)
  
  word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
  
  word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)
  
  return word_freq[:n]




#reading csv file into pandas DataFrame
data = pd.read_csv('papers_filtered.csv')

#calculating word count for each document
data['word_count'] = data['Abstract'].apply(lambda x: len(str(x).split(" ")))

#calculating freq of each word 
freq = pd.Series("".join(data['Abstract']).split()).value_counts()[:20]
print(freq)

# making a set of stop_words
stop_words = set(stopwords.words('english'))

new_words = ["using", "show", "result", "large", "also", "iv", "one", "two", "new", "previously", "shown"]

stop_words = stop_words.union(new_words)


#Vectorizing the corpus into ngram ranging from 0-3
cv = CountVectorizer(max_df = 0.8, stop_words = stop_words, max_features = 10000, ngram_range = (0,3))

X = cv.fit_transform(corpus)


#filtering stop_words,digits,special_characters and lemmatizing into dictionary words
corpus = []

for i in range(0,3924):
  text  = re.sub('^a-zA-Z', ' ', data['Abstract'][i])
  
  text = text.lower()
  
  text = re.sub('&lt;/?.*?&gt;', ' &lt;&gt; ', text)
  
  text=re.sub("(\\d|\\W)+"," ",text)
  
  text = text.split()
  
  lem = WordNetLemmatizer()
  
  text = [lem.lemmatize(word) for word in text if not word in stop_words]
  
  text = ' '.join(text)
  
  corpus.append(text)

print(list(cv.vocabulary_.items())[:10])

top_words = get_top_n_words(corpus, n = 20)

top_df = pd.DataFrame(top_words)

top_df.columns = ['word','freq']

#plotting top_n_words for ngram = 1
import seaborn as sns
sns.set(rc={'figure.figsize':(13,8)})
g = sns.barplot(x="word", y="freq", data=top_df)
g.set_xticklabels(g.get_xticklabels(), rotation=30)

top2_words = get_top_n2_words(corpus, n = 20)

top2_df = pd.DataFrame(top2_words)

top2_df.columns = ['word','freq']

#plotting top_n_words for ngram = 2
import seaborn as sns
sns.set(rc={'figure.figsize':(13,8)})
g = sns.barplot(x="word", y="freq", data=top2_df)
g.set_xticklabels(g.get_xticklabels(), rotation=30)

top3_words = get_top_n3_words(corpus, n = 20)

top3_df = pd.DataFrame(top3_words)

top3_df.columns = ['word','freq']

#plotting top_n_words for ngram = 3
import seaborn as sns
sns.set(rc={'figure.figsize':(13,8)})
g = sns.barplot(x="word", y="freq", data=top3_df)
g.set_xticklabels(g.get_xticklabels(), rotation=30)

# making TF-IDF vector for a doc at index 532
doc_index = 531

tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)
tfidf_transformer.fit(X)

#extracting feature names
feature_names = cv.get_feature_names()

doc = corpus[doc_index]

tfidf_vector = tfidf_transformer.transform(cv.transform([doc]))


sorted_items = sorted_coo(tfidf_vector.tocoo())

results = extract_topn_words(feature_names, sorted_items, 10)

#printing top k keywords
print("\nAbstract:")
print(doc)
print("\nKeywords:")
for k in results:
    print(k,results[k])