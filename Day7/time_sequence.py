# -*- coding: utf-8 -*-
"""time-sequence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--KgJWy9m62tvL9Kdj4vWJ7wSoh-FsMZ
"""

#authenticating google drive
!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#importing necessary libraries
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D
from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D
from keras.utils import np_utils

downloaded = drive.CreateFile({'id':''})
downloaded.GetContentFile('WISDM_ar_v1.1_raw.txt')

#function to convert into floating
def convert_to_float(x):

    try:
        return np.float(x)
    except:
        return np.nan

#reading the csv file and filtering ';' from 'z-axis' column with dropping Nan
def read_data(file):
  
  columns = ['user_id', 'activity', 'time_stamp', 'x-axis', 'y-axis', 'z-axis']
  
  df = pd.read_csv(file, header = None, names = columns)
  
  df['z-axis'].replace(regex = True,inplace = True,to_replace = r';',value = r'')
  
  df['z-axis'] = df['z-axis'].apply(convert_to_float)
  
  df.dropna(axis = 0, how = 'any', inplace = True)
  
  return df

# function to plot two variables x and y 
def plot_axis(ax, x, y, title):
  ax.plot(x, y)
  ax.set_title(title)
  ax.xaxis.set_visible(False)
  ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])
  ax.set_xlim([min(x), max(x)])
  ax.grid(True)

#function to plot axis values for each activity
def plot_activity(activity, data):
  fig, (ax0, ax1, ax2) = plt.subplots(nrows =3, figsize = (15, 10), sharex = True)
  
  plot_axis(ax0, data['time_stamp'], data['x-axis'], 'x-axis')
  plot_axis(ax1, data['time_stamp'], data['y-axis'], 'y-axis')
  plot_axis(ax2, data['time_stamp'], data['z-axis'], 'z-axis')
  plt.subplots_adjust(hspace = 0.2)
  fig.suptitle(activity)
  plt.subplots_adjust(top = 0.90)
  fig.savefig('sample_plot.png')
  plt.show()

# noamalizing the data with their mean and standard deviation
def feature_normalize(dataset):
  mu = np.mean(dataset, axis = 0)
  sigma = np.std(dataset, axis = 0)
  return (dataset - mu)/sigma

# making groups of time_steps for input to model returns training data and target variable
def create_segments_and_labels(df, time_steps, step, label_name):
  N_FEATURES = 3
  
  segments = []
  labels = []
  
  for i in range(0, len(df) - time_steps, step):
    xs = df['x-axis'].values[i: i + time_steps]
    ys = df['y-axis'].values[i: i + time_steps]
    zs = df['z-axis'].values[i: i + time_steps]
    
    label = stats.mode(df[label_name][i: i + time_steps])[0][0]
    segments.append([xs, ys, zs])
    labels.append(label)
    
  
  reshaped_segments = np.asarray(segments, dtype = np.float32).reshape(-1, time_steps, N_FEATURES)
  labels = np.asarray(labels)
  
  return reshaped_segments, labels

#reading the data
df = read_data('WISDM_ar_v1.1_raw.txt')

# plotting distribution of activities 
df['activity'].value_counts().plot(kind = 'bar', title = 'Training Examples by Activity Type')
plt.show()

# plotting distribution of users_id
df['user_id'].value_counts().plot(kind='bar',title= 'Training Examples by User')
plt.show()

# plotting each activity individually
for activity in np.unique(df['activity']):
  subset = df[df['activity'] == activity][:180]
  plot_activity(activity, subset)

# putting integer values for each activity label
LABEL = 'ActivityEncoded'

le = preprocessing.LabelEncoder()

df[LABEL] = le.fit_transform(df['activity'].values.ravel())

# divsion into training and testing set
df_test = df[df['user_id'] > 28]
df_train = df[df['user_id'] <= 28]

# Normalizing x, y and z axis variables
df_train['x-axis'] = feature_normalize(df['x-axis'])
df_train['y-axis'] = feature_normalize(df['y-axis'])
df_train['z-axis'] = feature_normalize(df['z-axis'])

# rounding floating numbers to 6 decimal places
df_train = df_train.round({'x-axis':6, 'y-axis':6, 'z-axis':6})


# creating training data and target variable
TIME_PERIODS = 80
STEP_DISTANCE = 40

x_train, y_train = create_segments_and_labels(df_train, TIME_PERIODS, STEP_DISTANCE, LABEL)

print(x_train.shape)

print(y_train.shape)

num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]
num_classes = le.classes_.size
print(list(le.classes_))

# reshaping data
input_shape = (num_time_periods * num_sensors)
x_train = x_train.reshape(x_train.shape[0],input_shape)

# converting the data into float32 format
x_train = x_train.astype('float32')
y_train = y_train.astype('float32')

# one-hot encoding for target variable
y_train = np_utils.to_categorical(y_train, num_classes)
print('New y_train.shape:', y_train.shape)

#model

model = Sequential()
model.add(Reshape((TIME_PERIODS, num_sensors), input_shape = (input_shape,)))
model.add(Conv1D(100, 10, activation = 'relu', input_shape = (TIME_PERIODS, num_sensors)))
model.add(Conv1D(100, 10, activation = 'relu'))
model.add(MaxPooling1D(3))
model.add(Conv1D(160, 10, activation = 'relu'))
model.add(Conv1D(160, 10, activation = 'relu'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation = 'softmax'))
print(model.summary())

# callbacks
callbacks_list = [ keras.callbacks.ModelCheckpoint(filepath = 'best_model.{epoch:02d}-{val_loss:.2f}.h5',
                                                  monitor = 'val_loss', save_best_only = True),
                 keras.callbacks.EarlyStopping(monitor = 'acc', patience = 1)]

# compilation
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# fitting the model 
BATCH_SIZE = 400
EPOCHS = 50

history = model.fit(x_train,
                   y_train,
                   batch_size = BATCH_SIZE,
                   epochs = EPOCHS,
                   callbacks = callbacks_list,
                   validation_split = 0.2,
                   verbose  =1)

# plotting training versus validation against loss and accuracy
fig = plt.figure(figsize=(6, 4))
plt.plot(history.history['acc'], "g--", label="Accuracy of training data")
plt.plot(history.history['val_acc'], "g", label="Accuracy of validation data")
plt.plot(history.history['loss'], "r--", label="Loss of training data")
plt.plot(history.history['val_loss'], "r", label="Loss of validation data")
plt.title('Model Accuracy and Loss')
plt.ylabel('Accuracy and Loss')
plt.xlabel('Training Epoch')
plt.ylim(0)
plt.legend()
fig.savefig('accuracy.png')
plt.show()

#preprocessing testing data
df_test = df_test.round({'x-axis':6,'y-axis':6,'z-axis':6})

x_test, y_test = create_segments_and_labels(df_test, TIME_PERIODS, STEP_DISTANCE, LABEL)

x_test = x_test.reshape(x_test.shape[0], input_shape)

x_test = x_test.astype('float32')
y_test = y_test.astype('float32')

y_test = np_utils.to_categorical(y_test, num_classes)

score = model.evaluate(x_test, y_test, verbose = 1)

# function to draw confusion matrix between actual and predicted
def show_confusion_matrix(validations, predictions):
  matrix = metrics.confusion_matrix(validations, predictions)
  
  fig = plt.figure(figsize = (6,4))
  sns.heatmap(matrix,
             cmap = 'coolwarm',
             linecolor = 'white',
             linewidths = 1,
             xticklabels = list(le.classes_),
             yticklabels = list(le.classes_),
             annot = True,
             fmt = 'd')
  plt.title('confusion matrix')
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')
  fig.savefig('confusion_matrix.png')
  plt.show()

# predicting for testing data
y_pred_test = model.predict(x_test)

max_y_pred_test = np.argmax(y_pred_test, axis = 1)

max_y_test = np.argmax(y_test, axis = 1)

# plotting confusion matrix
show_confusion_matrix(max_y_test, max_y_pred_test)

# Reviewing the scores
print(classification_report(max_y_test, max_y_pred_test))